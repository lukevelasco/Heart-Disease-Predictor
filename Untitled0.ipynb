{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNd3ZpyJPexLT+QoFjNiq/7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lukevelasco/Heart-Disease-Predictor/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "O6lMRJ9UMODG",
        "outputId": "8b5f1348-b301-476e-e3d8-9370c0745fbb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../lib/heart.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e10cf11e63fc>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#Loads dataset into df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../lib/heart.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../lib/heart.csv'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "#Loads dataset into df\n",
        "df = pd.read_csv(\"../lib/heart.csv\")\n",
        "\n",
        "df.head()\n",
        "df.shape\n",
        "\n",
        "df.sex.value_counts()\n",
        "\n",
        "#Countplot displaying \"Thal\" - A number that represents an assessment of blood flow during exercise\n",
        "sns.countplot(x=\"thal\", data=df, palette=\"bwr\")\n",
        "plt.show()\n",
        "\n",
        "#Countplot displaying the amount of males and females within a dataset\n",
        "sns.countplot(x=\"sex\", data = df, palette = \"mako_r\")\n",
        "plt.show()\n",
        "\n",
        "#Finds the amount of people within the dataset that have a slope of 2 (Downsloping slope of the peak)\n",
        "count2slope = len(df[df.slope == 2])\n",
        "print(\"Percentage of people that have a slope of 2: {:.2f}%\".format((count2slope/(len(df.slope)) * 100)))\n",
        "\n",
        "#Finds the amount of people that have a Fasting Blood Sugar <= 120\n",
        "count0fbs = len(df[df.fbs == 0])\n",
        "print(\"Percentage of people that have a Fasting Blood Sugar <= 120: {:.2f}%\".format((count0fbs/(len(df.fbs)) * 100)))\n",
        "\n",
        "#Finds the amount of people that have a Fasting Blood Sugar > 120\n",
        "count1fbs = len(df[df.fbs == 1])\n",
        "print(\"Percentage of people that have a Fasting Blood Sugar > 120: {:.2f}%\".format((count1fbs/(len(df.fbs)) * 100)))\n",
        "\n",
        "#Getting mean values of the each column within our dataset depending if a patient has heart disease or not\n",
        "df.groupby('target').mean()\n",
        "\n",
        "\n",
        "pd.crosstab(df.age, df.thal).plot(kind=\"bar\", figsize=(20, 6))\n",
        "plt.title(\"Thal Frequency for Ages\")\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "pd.crosstab(df.sex, df.target).plot(kind=\"line\", figsize=(15, 6))\n",
        "plt.title(\"Heart Disease dependent on Sex\")\n",
        "plt.xlabel('Sex')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "#The Slope of the ST Segment is positively correlated to having heart disease\n",
        "pd.crosstab(df.slope, df.target).plot(kind=\"bar\", figsize=(15, 6))\n",
        "plt.title(\"Heart Disease dependent on Slope of ST Segment\")\n",
        "plt.xlabel('Slope')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "#Based on our graph we can say that Fasting Blood Sugar doesn't correlate negatively or positively to having Heart Disease\n",
        "pd.crosstab(df.fbs, df.target).plot(kind=\"bar\", figsize=(15, 6), color=('black', 'red'))\n",
        "plt.title(\"Heart Disease dependent on Fasting Blood Sugar\")\n",
        "plt.xlabel('Fasting Blood Sugar')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend([\"No Heart Disease\", \"Has Heart Disease\"])\n",
        "plt.show()\n",
        "\n",
        "#Based on our graph we can say that Chest Pain type is positively correlated to having Heart Disease\n",
        "pd.crosstab(df.cp, df.target).plot(kind=\"bar\", figsize=(20, 6), color=('purple', 'yellow'))\n",
        "plt.title(\"Heart Disease dependent on Chest Pain Type\")\n",
        "plt.xlabel('Chest Pain Type')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend([\"No Heart Disease\", \"Has Heart Disease\"])\n",
        "plt.show()\n",
        "\n",
        "#Dummy Variables\n",
        "a = pd.get_dummies(df['cp'], prefix =\"cp\")\n",
        "b = pd.get_dummies(df['thal'], prefix = \"thal\")\n",
        "c = pd.get_dummies(df['slope'], prefix = \"slope\")\n",
        "\n",
        "frames = [df, a, b, c]\n",
        "df = pd.concat(frames, axis = 1)\n",
        "df.head()\n",
        "\n",
        "y = df.target.values\n",
        "x_data = df.drop(['target'], axis = 1)\n",
        "\n",
        "#Min-Max Normalization\n",
        "x = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data))\n",
        "\n",
        "#Split training and testing within our dataset\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=0)\n",
        "\n",
        "#Fit our training and testing\n",
        "x_train = x_train.T\n",
        "y_train = y_train.T\n",
        "x_test = x_test.T\n",
        "y_test = y_test.T\n",
        "\n",
        "#Sigmoid Function - Converts any number to go between 0 and 1\n",
        "#Equation - 1 / (1 + e^(-z))\n",
        "def sigmoid(z):\n",
        "  y_head = 1 / (1 + np.exp(-z))\n",
        "  return y_head\n",
        "\n",
        "#Forward and Backward Propagation Function\n",
        "def forwardBackward(weight, bias, x_train, y_train, lambda_reg=0.01):\n",
        "\n",
        "  #Forward aspect\n",
        "\n",
        "  y_head = sigmoid(np.dot(weight.T, x_train) + bias)\n",
        "  #The loss measures how well the predicted values are with the actual values\n",
        "  loss = -(y_train * np.log(y_head) + (1 - y_train) * np.log(1 - y_head))\n",
        "  #Calculates the total loss\n",
        "  cost = np.sum(loss) / x_train.shape[1] + (lambda_reg / 2) * np.sum(weight ** 2)\n",
        "\n",
        "  #Backward\n",
        "  #This line computes the gradient of the loss with respect to the weights. It tells us how much the loss would change if we made small adjustments to the weights.\n",
        "  derivative_weight = np.dot(x_train, ((y_head - y_train).T)) / x_train.shape[1] + lambda_reg * weight\n",
        "  #This line computes the gradient of the loss with respect to the bias term.\n",
        "  derivative_bias = np.sum(y_head - y_train) / x_train.shape[1]\n",
        "  gradients = {\"Derivative Weight\": derivative_weight, \"Derivative Bias\": derivative_bias}\n",
        "\n",
        "  return cost, gradients\n",
        "\n",
        "#Prediction Function\n",
        "def predict(weight, bias, x_test):\n",
        "  z = np.dot(weight.T,x_test) + bias\n",
        "  y_head = sigmoid(z)\n",
        "  y_prediction = np.zeros((1, x_test.shape[1]))\n",
        "\n",
        "  for i in range(y_head.shape[1]):\n",
        "    if y_head[0, i] <= 0.5:\n",
        "      y_prediction[0, i] = 0\n",
        "    else:\n",
        "      y_prediction[0, i] = 1\n",
        "\n",
        "  return y_prediction\n",
        "\n",
        "#Initializes weight and bias for our model\n",
        "def initialize(dimension):\n",
        "  weight = np.full((dimension, 1), 0.01)\n",
        "  bias = 0.0\n",
        "  return weight, bias\n",
        "\n",
        "#Helps improve model performance and optimization process by standardizing features (all features have similar ranges)\n",
        "#Allows our gradient descent algorithm to converge faster\n",
        "def scale_features(x_train, x_test):\n",
        "  scaler = StandardScaler()\n",
        "  x_train_scaled = scaler.fit_transform(x_train.T).T\n",
        "  x_test_scaled = scaler.transform(x_test.T).T\n",
        "  return x_train_scaled, x_test_scaled\n",
        "\n",
        "#Updates model weights and bias through gradient descent with regularization.\n",
        "def update(weight, bias, x_train, y_train, learningRate, iteration, lambda_reg=0.01):\n",
        "  costList = []\n",
        "  index = []\n",
        "\n",
        "  #Adjust the weights and bias for each iteration\n",
        "  for i in range(iteration):\n",
        "    cost, gradients = forwardBackward(weight, bias, x_train, y_train, lambda_reg)\n",
        "\n",
        "    #Multiplying by Learning Rate allows model to not adjust too quickly\n",
        "    weight -= learningRate * gradients[\"Derivative Weight\"]\n",
        "    bias -= learningRate * gradients[\"Derivative Bias\"]\n",
        "\n",
        "    costList.append(cost)\n",
        "    index.append(i)\n",
        "\n",
        "  parameters = {\"weight\": weight, \"bias\": bias}\n",
        "\n",
        "  print(\"Iterations: \", iteration)\n",
        "  print(\"Costs:\", cost)\n",
        "\n",
        "  #Shows how the cost function changes as the weights and bias's are updated over iterations\n",
        "  #Future - if the cost function is decreasing, our model is improving it's \"fit\" to our data - making better predictions\n",
        "  plt.plot(index, costList)\n",
        "  plt.xlabel(\"Number of Iterations\")\n",
        "  plt.ylabel(\"Cost\")\n",
        "  plt.show()\n",
        "\n",
        "  return parameters, gradients\n",
        "\n",
        "  #Logistic Regression Function\n",
        "def logistic_regression(x_train, y_train, x_test, y_test, learningRate, iterations, lambda_reg=0.01):\n",
        "\n",
        "  #Scale features\n",
        "  x_train, x_test = scale_features(x_train, x_test)\n",
        "\n",
        "  #Initialize Parameters\n",
        "  dimension = x_train.shape[0]\n",
        "  weight, bias = initialize(dimension)\n",
        "\n",
        "  parameters, gradient = update(weight, bias, x_train, y_train, learningRate, iterations, lambda_reg=0.01)\n",
        "\n",
        "  #Make Predictions on the test set\n",
        "  y_prediction = predict(parameters[\"weight\"], parameters[\"bias\"], x_test)\n",
        "\n",
        "  #Convert the probabilities into binary\n",
        "  y_prediction_binary = (y_prediction > 0.5).astype(int)\n",
        "\n",
        "  #Calculate the accuracy\n",
        "  accuracy = np.mean(y_prediction_binary == y_test)\n",
        "  print(\"Manual Test Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "\n",
        "logistic_regression(x_train, y_train, x_test, y_test, learningRate=0.01, iterations=100, lambda_reg=0.01)\n",
        "\n",
        "#Map to hold the accuracies of all of our models\n",
        "accuracies = {}\n",
        "\n",
        "#Use LogisticRegression Class to see it's accuracy\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x_train.T, y_train.T)\n",
        "acc = lr.score(x_test.T, y_test.T) * 100\n",
        "\n",
        "accuracies['Logistic Regression'] = acc\n",
        "print(\"Test Accuracy {:.2f}%\".format(acc))\n",
        "\n",
        "x_train = x_train.T\n",
        "y_train = y_train.T\n",
        "x_test = x_test.T\n",
        "y_test = y_test.T\n",
        "\n",
        "#Model Steps\n",
        "'''\n",
        "1. Import the model\n",
        "2. Initialize Model\n",
        "3. Fit the model to our training data\n",
        "4. Get a score (accuracy) on how well our model did with the test data\n",
        "'''\n",
        "\n",
        "#Decision Tree Classifier - Continuously ask questions until you get the answer\n",
        "dtc = DecisionTreeClassifier()\n",
        "dtc.fit(x_train, y_train)\n",
        "acc = dtc.score(x_test, y_test) * 100\n",
        "accuracies['Decision Tree'] = acc\n",
        "print(\"Decision Tree Accuracy: {:.2f}%\".format(acc))\n",
        "\n",
        "#Random Forest Classifier - A bunch of decision trees (forest of decision trees)\n",
        "rf = RandomForestClassifier(n_estimators=1000, random_state=1)\n",
        "rf.fit(x_train, y_train)\n",
        "acc = rf.score(x_test, y_test) * 100\n",
        "accuracies['Random Forest'] = acc\n",
        "print(\"Random Forest Classifier Accuracy: {:.2f}%\".format(acc))\n",
        "\n",
        "#KNN Model - Finding closest neighbors\n",
        "knn = KNeighborsClassifier(n_neighbors = 2)\n",
        "knn.fit(x_train, y_train)\n",
        "prediction = knn.predict(x_test)\n",
        "acc = knn.score(x_test, y_test) * 100\n",
        "\n",
        "scoreList = [] #Hold the accuracies of the model depending on how many neighbors we look at\n",
        "for i in range(1, 20):\n",
        "  knn2 = KNeighborsClassifier(n_neighbors = i)\n",
        "  knn2.fit(x_train, y_train)\n",
        "  scoreList.append(knn2.score(x_test, y_test) * 100)\n",
        "\n",
        "print(\"KNN Model Accuracy: {:.2f}%\".format(acc))\n",
        "\n",
        "plt.plot(range(1, 20), scoreList)\n",
        "plt.xticks(np.arange(1, 20, 1))\n",
        "plt.xlabel(\"K-Value\")\n",
        "plt.ylabel(\"Accuracy Percentage\")\n",
        "plt.show()\n",
        "\n",
        "accuracies['KNN Model'] = acc\n",
        "\n",
        "#Support Vector Machine (SVM) Algorithm\n",
        "svm = SVC(random_state = 1)\n",
        "svm.fit(x_train, y_train)\n",
        "acc = svm.score(x_test, y_test) * 100\n",
        "accuracies['SVM'] = acc\n",
        "print(\"SVM Algorithm Accuracy: {:.2f}%\".format(acc))\n",
        "\n",
        "#Naive Bayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(x_train, y_train)\n",
        "acc = nb.score(x_test, y_test) * 100\n",
        "accuracies['Naive Bayes'] = acc\n",
        "print(\"Naive Bayes Accuracy: {:.2f}%\".format(acc))\n",
        "\n",
        "colors = [\"red\", \"yellow\", \"orange\", \"green\", \"blue\", \"purple\"]\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize=(16,5))\n",
        "plt.yticks(np.arange(0, 100, 10))\n",
        "plt.xlabel(\"Types of Algorithms/Models\")\n",
        "plt.ylabel(\"Accuracy Percentage (%)\")\n",
        "sns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)\n",
        "plt.show()\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(x, y, test_size=0.2, random_state=None)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=None)\n",
        "\n",
        "#Create a sequential model - one input and one output\n",
        "model = Sequential()\n",
        "\n",
        "#Input Layer\n",
        "model.add(Input(shape=(24,)))\n",
        "\n",
        "#Hidden Layers\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "#Output Layer\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#If the model doesn't improve loss in 10 consecutive epochs then stop it\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "#Evaluate the model on the test data\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Test Accuracy: {test_accuracy}')\n",
        "\n",
        "# Make predictions\n",
        "y_pred_proba = model.predict(X_test)\n",
        "y_pred = (y_pred_proba.flatten() > 0.5).astype(int)\n",
        "\n",
        "#Print predictions and true values\n",
        "print(f'Predictions: {y_pred}')\n",
        "print(f'True Values: {y_test.flatten()}')\n"
      ]
    }
  ]
}